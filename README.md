# Azure Data Engineering Project - Adventure Works Analytics

## ğŸ¯ Project Overview

This end-to-end Azure Data Engineering project demonstrates a complete **Medallion Architecture** implementation using Adventure Works dataset. The project showcases real world data engineering scenarios including API integration, dynamic pipeline creation, and advanced data transformations.

**ğŸ”— [Project Demo Video]([Insert Video Link Here])**

![Project Architecture](images/architecture-diagram.png)

## ğŸ—ï¸ Architecture

This project implements a **Bronze â†’ Silver â†’ Gold** medallion architecture pattern:

- **Bronze Layer (Raw)**: Ingested data from GitHub API without transformations
- **Silver Layer (Transformed)**: Cleaned and transformed data with business logic
- **Gold Layer (Serving)**: Analytics-ready data warehouse for consumption

![Data Flow](images/data-flow-diagram.png)

## ğŸ› ï¸ Tech Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Orchestration** | Azure Data Factory | ETL/ELT pipeline orchestration |
| **Data Processing** | Azure Databricks (PySpark) | Big data transformations |
| **Data Storage** | Azure Data Lake Gen2 | Scalable data lake storage |
| **Data Warehouse** | Azure Synapse Analytics | Serverless SQL analytics |
| **Visualization** | Power BI | Business intelligence dashboards |
| **Security** | Service Principal & Managed Identity | Secure authentication |

## ğŸ“Š Dataset

**Adventure Works Sales Dataset** - A comprehensive business dataset containing:
- ğŸ“… 3 years of sales data (2015-2017)
- ğŸ›ï¸ Products, categories, and subcategories
- ğŸ‘¥ Customer demographics and territories  
- ğŸ“¦ Returns and order information
- ğŸ—“ï¸ Calendar dimension data

![Dataset Overview](images/dataset-overview.png)

## ğŸš€ Key Features

### âœ¨ Real-World Scenarios Implemented

- **Dynamic Pipeline Creation**: Parameter-driven pipelines using For-Each loops
- **API Data Ingestion**: Direct data extraction from GitHub repository
- **Advanced PySpark Transformations**: Date functions, string manipulations, aggregations
- **External Table Creation**: Three-step external table setup in Synapse
- **End-to-End Security**: Service Principal and Managed Identity implementation

![Dynamic Pipeline](images/dynamic-pipeline.png)

### ğŸ”§ Advanced Transformations

```python
# Date transformations
df_cal = df_cal.withColumn('Month', month(col('Date')))
df_cal = df_cal.withColumn('Year', year(col('Date')))

# String manipulations  
df_cus.withColumn('FullName', concat_ws(' ', col('Prefix'), col('FirstName'), col('LastName')))

# Data aggregations
df_sales.groupBy('OrderDate').agg(count('OrderNumber').alias('TotalOrders'))
```

![Data Transformations](images/transformations-code.png)

## ğŸ›ï¸ Project Structure

```
azure-data-engineering-project/
â”œâ”€â”€ ğŸ“ data-factory/
â”‚   â”œâ”€â”€ pipelines/
â”‚   â”‚   â”œâ”€â”€ main-pipeline.json
â”‚   â”‚   â””â”€â”€ dynamic-pipeline.json
â”‚   â”œâ”€â”€ datasets/
â”‚   â”‚   â”œâ”€â”€ ds_http.json
â”‚   â”‚   â””â”€â”€ ds_raw.json
â”‚   â””â”€â”€ linkedservices/
â”œâ”€â”€ ğŸ“ databricks/
â”‚   â”œâ”€â”€ notebooks/
â”‚   â”‚   â”œâ”€â”€ bronze-to-silver.py
â”‚   â”‚   â””â”€â”€ data-transformations.py
â”‚   â””â”€â”€ configs/
â”‚       â””â”€â”€ storage-config.py
â”œâ”€â”€ ğŸ“ synapse/
â”‚   â”œâ”€â”€ sql-scripts/
â”‚   â”‚   â”œâ”€â”€ create-external-tables.sql
â”‚   â”‚   â”œâ”€â”€ create-views.sql
â”‚   â”‚   â””â”€â”€ setup-credentials.sql
â”‚   â””â”€â”€ schemas/
â”œâ”€â”€ ğŸ“ powerbi/
â”‚   â””â”€â”€ adventure-works-dashboard.pbix
â”œâ”€â”€ ğŸ“ configs/
â”‚   â””â”€â”€ pipeline-config.json
â””â”€â”€ ğŸ“ images/
    â””â”€â”€ [Architecture diagrams and screenshots]
```

## âš™ï¸ Azure Resources Created

| Resource | Name | Purpose |
|----------|------|---------|
| **Resource Group** | `AdventureWorksProject` | Container for all resources |
| **Storage Account** | `adevnturestorage` | Data Lake Gen2 storage |
| **Data Factory** | `adf-adventure-works-project` | ETL orchestration |
| **Databricks** | `adb-aw-project` | Data processing workspace |
| **Synapse Analytics** | `synapse-aw-project` | Data warehouse |
| **App Registration** | `awproject_app` | Service Principal for security |

![Azure Resources](images/azure-resources.png)

## ğŸ” Security Implementation

### Service Principal Setup
```python
# Databricks storage access configuration
spark.conf.set("fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net", "OAuth")
spark.conf.set("fs.azure.account.oauth.provider.type.<storage-account>.dfs.core.windows.net", 
               "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider")
spark.conf.set("fs.azure.account.oauth2.client.id.<storage-account>.dfs.core.windows.net", 
               "<application-id>")
spark.conf.set("fs.azure.account.oauth2.client.secret.<storage-account>.dfs.core.windows.net", 
               "<client-secret>")
```

![Security Setup](images/security-setup.png)

## ğŸ“ˆ Data Pipeline Flow

### Phase 1: Data Ingestion (Bronze Layer)
![Bronze Layer](images/bronze-layer-ingestion.png)

**Dynamic Pipeline Implementation:**
- JSON configuration-driven data ingestion
- For-Each loop for multiple file processing
- HTTP source to Data Lake sink mapping

### Phase 2: Data Transformation (Silver Layer)  
![Silver Layer](images/silver-layer-transformation.png)

**PySpark Transformations:**
- Date dimension creation
- Customer name standardization
- Product SKU cleansing
- Sales metrics calculation

### Phase 3: Data Serving (Gold Layer)
![Gold Layer](images/gold-layer-serving.png)

**Synapse External Tables:**
```sql
-- Create external table in 3 steps
CREATE DATABASE SCOPED CREDENTIAL credential_name
WITH IDENTITY = 'Managed Identity';

CREATE EXTERNAL DATA SOURCE source_name
WITH (LOCATION = 'abfss://silver@storage.dfs.core.windows.net/',
      CREDENTIAL = credential_name);

CREATE EXTERNAL FILE FORMAT parquet_format
WITH (FORMAT_TYPE = PARQUET);
```

## ğŸ“Š Analytics & Visualization

![Power BI Dashboard](images/powerbi-dashboard.png)

**Key Metrics Tracked:**
- ğŸ“ˆ Daily sales trends
- ğŸ† Top performing products
- ğŸŒ Regional sales analysis
- ğŸ“… Seasonal patterns
- ğŸ”„ Return analysis

## ğŸš€ Getting Started

### Prerequisites
- Azure Subscription (Free tier available)
- Power BI Desktop
- Basic knowledge of SQL and Python

### Setup Instructions

1. **Clone the Repository**
   ```bash
   git clone https://github.com/[your-username]/azure-data-engineering-project.git
   cd azure-data-engineering-project
   ```

2. **Deploy Azure Resources**
   ```bash
   # Create Resource Group
   az group create --name AdventureWorksProject --location eastus
   
   # Deploy resources using ARM template
   az deployment group create --resource-group AdventureWorksProject --template-file deploy/main.json
   ```

3. **Configure Data Factory**
   - Import pipeline definitions from `data-factory/` folder
   - Update connection strings and credentials

4. **Setup Databricks**
   - Import notebooks from `databricks/notebooks/`
   - Configure cluster and storage access

5. **Deploy Synapse Objects**
   - Execute SQL scripts from `synapse/sql-scripts/`
   - Create external tables and views

![Setup Process](images/setup-process.png)

## ğŸ’¡ Key Learnings & Interview Points

### ğŸ¯ Technical Concepts Mastered

- **Medallion Architecture**: Bronze-Silver-Gold data layering
- **Dynamic Pipelines**: Parameter-driven ADF pipelines
- **PySpark Optimization**: Efficient data transformations
- **External Tables**: Synapse Analytics data virtualization
- **Security Best Practices**: Service Principal implementation

### ğŸ™ï¸ Interview Questions Covered

1. **"How do you handle dynamic data ingestion?"**
   - Implemented For-Each loops with JSON configuration
   - Parameter-driven pipeline design

2. **"Explain your data transformation approach"**
   - Medallion architecture with clear separation of concerns
   - PySpark transformations with built-in functions

3. **"How do you ensure data security in Azure?"**
   - Service Principal for cross-service authentication
   - Managed Identity for Synapse-Data Lake integration

![Interview Prep](images/interview-points.png)

## ğŸ”§ Troubleshooting Guide

### Common Issues & Solutions

| Issue | Solution |
|-------|----------|
| **Storage Access Denied** | Verify Service Principal permissions |
| **Databricks Cluster Failed** | Check cluster size and region availability |
| **Synapse Query Timeout** | Optimize query with proper filtering |
| **Power BI Connection Error** | Validate SQL endpoint and credentials |

## ğŸ“š Additional Resources

- ğŸ“– [Project Documentation](docs/)
- ğŸ¥ [Video Walkthrough]([Video Link])
- ğŸ“ [Medium Article]([Medium Link])
- ğŸ’» [LinkedIn Post]([LinkedIn Link])

## ğŸ¤ Contributing

Feel free to fork this project and submit pull requests for improvements!

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¨â€ğŸ’» Author

**[Your Name]**
- ğŸ“§ Email: [your.email@domain.com]
- ğŸ’¼ LinkedIn: [Your LinkedIn Profile]
- ğŸ± GitHub: [Your GitHub Profile]

---

â­ **If this project helped you, please give it a star!** â­

![Project Success](images/project-success.png)

> *"This project demonstrates end-to-end Azure Data Engineering capabilities from data ingestion to visualization, showcasing real-world scenarios and best practices."*
